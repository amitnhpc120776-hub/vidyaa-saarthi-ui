Table of Contents
Text Personalization	2
Input Capture	2
Analyze Text Complexity	2
Decide Personalization Strategy	3
Generate Personalized Text with LLM	4
Prompt template (guided hybrid, recommended for MVP):	4
Leveraging Personalized Text for Layered Learning	6
Interactive Learning Experience	8
Clarification Questions and Responses	8
Highlight & Expand	9
Quick Feedback Mechanism	11
Derivative Learning Resources	11
Study Notes	11
Visual Snippets	13
Flashcards	15
Practice Quizzes (Topic/Subtopic Level)	16
Practice Quizzes (Chapter Level)	18

 
Text Personalization
Every learner in VidyaaSarthi engages with the same NCERT/ICSE/State Board textbook, yet comprehension varies due to differences in working memory, attention, processing speed, and other cognitive traits. To address this, VidyaaSarthi re-expresses textbook passages into personalized explanations that remain curriculum-faithful but are adapted in readability, structure, and style for each learner.
Input Capture
The process begins when a learner selects their Class → Board → Subject → Chapter → Topic. VidyaaSarthi retrieves the corresponding textbook passage from the syllabus database, ensuring fidelity to the prescribed curriculum. Simultaneously, the learner’s cognitive profile (six traits: WMC, ATT, IPS, META, IRA, ALS), previously generated through profiling tests, is retrieved from the profile database. These two inputs — the source passage and learner profile — drive all personalization.
Analyze Text Complexity
Once the passage is identified, VidyaaSarthi performs a neutral diagnostic step to measure its linguistic complexity. This is not for restricting access (since learners can only view grade-appropriate material), but for determining the level of simplification required.
The primary tool is the Python library textstat, which provides multiple readability metrics:
•	Flesch Reading Ease
•	Flesch-Kincaid Grade Level (FKG)
•	Gunning Fog Index
•	SMOG Index
•	Automated Readability Index (ARI)
•	Dale–Chall Score
Illustrative snippet:
import textstat
text = "Photosynthesis is a physio-chemical process by which autotrophs synthesize carbohydrates."
print("Flesch Reading Ease:", textstat.flesch_reading_ease(text))
print("Flesch-Kincaid Grade:", textstat.flesch_kincaid_grade(text))
Sample output:
Flesch Reading Ease: 27.6  
Flesch-Kincaid Grade: 12.0
This diagnostic output is stored in the Content Analytics Database with fields such as topic_id, fkg_score, readability_band, and timestamp. On retrieval, the scores are compared with an internally defined target band for that grade.
Simplification cue logic (pseudocode):
diagnosed_fkg = <numeric output from textstat>  
target_band   = <min_fkg, max_fkg> for learner's grade  
target_mid    = average(target_band)  
gap           = diagnosed_fkg - target_mid  

if gap >= 3.0:
    simplification_hint = "Simplify aggressively: break into very short sentences, step-by-step logic, avoid jargon."
elif gap >= 2.0:
    simplification_hint = "Simplify moderately: simpler words, one example, concise sentences."
elif gap >= -1.0:  # within or slightly above band
    simplification_hint = "Simplify lightly: minor rephrasing, preserve detail."
else:  # below band
    simplification_hint = "Enrich slightly: add clarifications or one higher-order connection."
The simplification_hint is cached and later merged into the Final Cue for LLM prompting.
Decide Personalization Strategy
The system now layers in cognitive trait adjustments. Each trait corresponds to explicit delivery preferences:
•	Low WMC → short chunks, repeated keywords.
•	Weak ATT → highlight key terms, reinforce core ideas twice.
•	Slow IPS → single-step statements, avoid multi-clause sentences.
•	High META → add reflective prompts.
•	IRA support → add recall cues.
•	Analogy-driven ALS → inject one analogy from selected domains (sports, art, daily life).
These adjustments are expressed as structured strings (e.g., “Use short chunks. Highlight keywords. Add reflective question.”) and cached with the learner’s profile snapshot.
Generate Personalized Text with LLM
The Final Cue is constructed by concatenating:
•	The simplification_hint from text diagnostics.
•	The trait-based adjustments from the learner profile.
•	An optional interest-based instruction if the learner has opted for analogy hooks.
Illustrative construction:
Final Cue = simplification_hint 
          + " " 
          + join(trait_cues, " ") 
          + " Where appropriate, include one analogy from <interest>."
This cue, together with the original passage and diagnostic scores, is passed to the LLM.
Prompt template (guided hybrid, recommended for MVP):
Readability: FKG=<diagnosed_fkg>.  
Target band: <min_fkg>–<max_fkg>.  
Instruction: <Final Cue>.  
Constraints: Preserve syllabus fidelity and key terms; remain age-appropriate; avoid hallucinations.  
Passage: <original syllabus text>
The LLM returns the personalized explanation, which is written into the Personalized Content Database with fields for topic_id, learner_id, explanation_text, final_cue, and timestamp.
Database Interactions
•	Stored: readability diagnostics, simplification cue, Final Cue, personalized explanation.
•	Retrieved: personalized explanation for learner dashboard, Final Cue for auditability, diagnostics for analytics.
Libraries and Tools
•	Text complexity: textstat (primary), readability, spaCy (alternatives).
•	LLM interaction: openai (direct API), langchain (if chaining).
•	Data validation: nltk, spaCy for keyword/term presence checks.
•	Database: structured storage of cues, explanations, diagnostics.

 
Leveraging Personalized Text for Layered Learning
The output of VidyaaSarthi’s text personalization pipeline is a personalized explanation of the selected passage, tuned to the learner’s readability band and cognitive profile. However, this output is not intended to exist in isolation. Instead, it becomes the foundation for layered learning resources and interactions that extend beyond a single explanation, transforming a static passage into a versatile ecosystem of learning aids.
Derivative Learning Resources
From each personalized explanation, VidyaaSarthi can automatically generate a variety of supplemental resources:
•	Study Notes: concise bullet-point takeaways highlighting the three to five most essential ideas.
•	Slides or Visual Snippets: simplified visual representations that allow for quick revision, particularly useful for subjects like science and mathematics.
•	Flashcards: Q/A pairs derived from the explanation, designed for active recall.
•	Mnemonics: memory aids for equations, lists, or processes that are traditionally difficult to retain.
•	Practice Quizzes: two to three short MCQs or true/false items embedded directly from the personalized text, providing immediate opportunities for self-assessment.
•	Summary Reports: generated teacher/parent digests that present what was studied and highlight areas of difficulty or strength.
Each of these resources is anchored in the same syllabus passage, ensuring fidelity while diversifying the learner’s modes of engagement.
Interactive Learning Experience
To maximize the utility of the personalized text, the learner interface is designed to support multiple modes of interaction:
•	Highlight and Expand: learners can click on a confusing phrase within the personalized explanation to trigger a micro-explanation or simpler analogy.
•	Toggle Views: a switch between the original textbook passage and the rewritten personalized text, reinforcing comprehension and building confidence.
•	Feedback Loop: simple mechanisms (thumbs up/down, “Too Hard / Just Right / Too Easy”) allow learners to provide input, which in turn informs further adjustments in personalization.
User Interface Considerations
On larger screens such as desktops, VidyaaSarthi presents the original textbook passage and the personalized explanation side-by-side, with adjacent controls for generating study notes, quizzes, and reports. On mobile devices, where space is constrained, the same experience is restructured as stacked or tabbed views:
•	Tabbed Interface: separate tabs for Original Text, Personalized Explanation, Notes/Quizzes, and Resources.
•	Accordion Layout: collapsible sections that the learner can expand or contract, maintaining clarity in a limited display area.
•	Swipeable Cards: explanations, quizzes, mnemonics, and summaries presented as cards that can be swiped horizontally for intuitive navigation.
This adaptive interface ensures that learners receive a consistent experience across devices without compromising usability.
 
Interactive Learning Experience
Building on the personalized explanation, VidyaaSarthi incorporates lightweight interaction features that enable learners to engage with the content dynamically. These interactions are implemented in a manner that is uniform across the web platform and adapted for constrained mobile displays.
Clarification Questions and Responses
 In addition to consuming the personalized explanation and derivative learning resources, learners may pose free-form clarification questions. VidyaaSarthi addresses this need through a structured Clarification Engine that combines database-first retrieval with selective LLM fallback. The design principle is explicit: minimize real-time model usage and maximize reuse of previously logged responses.
When a learner submits a clarification question, the system first queries the Clarification Log Database, which stores historical question–answer pairs. Retrieval proceeds in two stages:
1.	Fuzzy text matching (rapidfuzz, difflib) to capture near-duplicate phrasing.
2.	Semantic similarity search, using pre-computed embeddings from sentence-transformers or OpenAI’s text-embedding-ada-002.
If a sufficiently similar match exists above the defined similarity threshold, the stored answer is surfaced instantly. This ensures that common questions are answered without invoking the LLM.
If no satisfactory match is found, the system triggers an LLM call. The input prompt includes:
•	The original textbook passage.
•	The personalized explanation.
•	The learner’s cognitive profile snapshot.
•	The submitted question.
Prompt structure:
Provide a concise clarification to the learner’s question.  
Constraints:  
- Stay strictly syllabus-faithful.  
- Use simple, grade-appropriate language.  
- Include 2–3 sentences only.  
Question: <learner_question>
The LLM’s response is written back into the Clarification Log Database along with:
•	question_text
•	answer_text
•	embedding_vector
•	topic_id / chapter_id
•	learner_class / board
•	timestamp
This ensures that future learners with the same or similar question will be served the cached answer.
The Clarification Engine thus operates as a two-tier workflow:
1.	Database-first retrieval → immediate response from logged Q&A pairs.
2.	LLM fallback → triggered only when no adequate match is found; new responses are cached for future reuse.
On the learner interface, clarifications are displayed as an expandable thread beneath the personalized explanation. On the web platform, this appears in a side panel with search and scroll functions. On mobile, it is rendered as a vertically expandable list of questions and answers. Learners can browse their own clarifications as well as previously logged clarifications linked to the same topic, creating a transparent, cumulative knowledge trail.
By logging and reusing all clarifications, VidyaaSarthi ensures that dependency on LLMs reduces steadily while coverage improves. Over time, the system builds a topic-specific FAQ repository, tightly grounded in the textbook and aligned with the learner’s profile, thereby turning every interaction into a reusable knowledge asset
Highlight & Expand
The Highlight & Expand feature allows learners to interact directly with the personalized explanation by selecting a sentence or phrase and requesting additional clarity. This transforms the explanation into a dynamic resource, where learners can dive deeper into only those parts they find confusing.
When a learner highlights text, the system performs sentence segmentation using spaCy or nltk to isolate the exact unit. The selected sentence is logged with metadata (learner_id, topic_id, sentence_text, timestamp). The system then queries the Clarification Log Database to check whether a micro-explanation for this sentence already exists.
•	If found: the stored clarification is retrieved and displayed instantly.
•	If not found: the system triggers an LLM call.
Prompt structure for new clarifications:
Simplify and expand the following sentence for the learner’s profile.  
Constraints:  
- Provide 1–2 additional sentences only.  
- Use simple, grade-appropriate language.  
- Include an optional analogy if appropriate.  
Sentence: <highlighted_text>
The generated micro-explanation is stored in the Clarification Log Database with fields:
•	sentence_id (linking to explanation sentence).
•	clarification_text.
•	learner_profile_snapshot.
•	timestamp.
On the web interface, clarifications appear as expandable tooltips or a side panel when hovering/clicking the highlighted text. On mobile devices, clarifications are triggered by long-press and displayed in a bottom-sheet modal, optimized for limited screen size.
Technical Flow
•	Libraries: spaCy (sentence segmentation), nltk (tokenization).
•	LLM Interaction: openai API or langchain pipeline for generating clarifications.
•	Database Storage: Clarification Log with sentence_id → clarification_text mapping for retrieval.
•	Retrieval logic: Database check first → fallback to LLM → store response for reuse.
By caching all clarifications at the sentence level, VidyaaSarthi minimizes repeated LLM calls, reduces latency, and steadily grows a reusable library of micro-explanations tied directly to the original textbook passages.

Quick Feedback Mechanism
The Quick Feedback Mechanism captures learner perceptions of the personalized explanation through simple, low-friction inputs. After reading the explanation, learners can provide an immediate response via options such as “Too Hard / Just Right / Too Easy” or a binary thumbs up/down.
On the web platform, these are displayed as inline buttons beneath the explanation card. On mobile, they are presented as a compact three-button strip positioned immediately after the explanation.
All feedback is logged in the Feedback Database, with fields:
•	learner_id
•	topic_id
•	explanation_id
•	feedback_value (e.g., Too Hard / Just Right / Too Easy)
•	timestamp
This mechanism requires no real-time LLM processing. Instead, feedback data is retrieved during analytics runs to inform iterative refinement of personalization strategies. For instance, if multiple learners consistently mark a passage as “Too Hard” despite being on-grade, VidyaaSarthi can adjust thresholds in the simplification cue logic or flag the passage for content review.
From an implementation standpoint:
•	Libraries/Tools: handled primarily via the frontend (React for web, Flutter/React Native for mobile). Backend capture via Flask/FastAPI routes.
•	Database: structured storage of individual feedback events, supporting aggregation at the learner, topic, or chapter level.
•	Processing: periodic aggregation jobs compute distributions of feedback (e.g., % of learners marking “Too Hard” for a topic).
By making learner input lightweight and consistently logged, the Quick Feedback Mechanism creates a feedback loop between learner experience and system adaptation, without incurring additional LLM costs.
Derivative Learning Resources
Study Notes
Study Notes
From each personalized explanation, VidyaaSarthi generates concise study notes that distill the content into three to five essential points. These serve as quick-reference material for revision and reinforcement, complementing the full explanation without duplicating it.
Topic-Level Generation
Study notes are anchored at the topic or subtopic level. When a learner views a personalized explanation, the text is passed through a summarization pipeline. For MVP, this is powered by an LLM with tightly controlled instructions:
•	Preserve syllabus fidelity.
•	Limit output to short bullet-style items.
•	Retain technical terms.
•	Remove redundancy.
Prompt structure:
Summarize the following explanation into 3–5 concise bullet points 
suitable for learners at this grade level. 
Constraints: 
- Preserve all key scientific terms. 
- Avoid duplication. 
- Use short, clear sentences.
Explanation: <personalized_explanation>
The generated notes are cached in the Notes Database with schema fields:
•	topic_id (linking notes to the textbook passage).
•	personalized_explanation_id (to ensure alignment with explanation version).
•	notes_text (the generated bullets).
•	Metadata such as learner_id, timestamp, and cognitive_profile_snapshot.
When the learner revisits the passage, cached notes are displayed directly, avoiding repeat LLM calls. If the learner’s profile changes significantly (e.g., after re-profiling), the notes may be refreshed on demand.
Chapter-Level Aggregation
At the chapter level, VidyaaSarthi does not generate notes independently. Instead, it aggregates topic-level notes across all subtopics. These may be:
•	Displayed as-is for comprehensive coverage.
•	Re-summarized via a lightweight LLM call to produce a concise chapter digest.
This approach ensures efficiency by leveraging already-generated notes, while still providing learners with structured summaries for revision at the end of a chapter.
Technical Approach
•	Best fit for MVP: LLM summarization with caching.
•	Validation: nltk and spaCy for coverage of key terms.
•	Alternatives: HuggingFace models such as bart or t5 for extractive summarization in future iterations.
•	Database: Notes stored as JSON bullets linked to topic_id and chapter_id, retrievable for dashboards and reports.
Interface
On the web platform, study notes are shown as a collapsible bulleted card aligned with the explanation panel. On mobile devices, they appear as a stacked “Key Points” card optimized for small screens. Learners can also save notes into a personalized notebook, creating an evolving archive of essential concepts at both topic and chapter levels.
Visual Snippets
In addition to textual study notes, VidyaaSarthi generates visual snippets that provide graphical representations of the personalized explanation. These visuals act as conceptual anchors, particularly in science and mathematics, where diagrams or process flows simplify complex relationships.
Generation Process
The pipeline begins with the personalized explanation text, which is parsed to identify entities (concepts, processes, relationships) and their dependencies. Tools such as spaCy or nltk are used for entity recognition and clause analysis. Heuristic rules determine which segments are suitable for visualization, e.g., sequences, cause–effect chains, or input–output transformations.
Once candidate structures are identified, the system constructs a visual schema. At MVP stage, this is generated through an LLM prompt:
From the following explanation, generate a diagram specification that shows the relationships between key entities. 
Constraints:
- Output in Mermaid.js format.
- Limit to 4–6 nodes for simplicity.
- Preserve technical accuracy.
Explanation: <personalized_explanation>
The LLM does not return a raw image but a structured description (e.g., Mermaid.js syntax, Graphviz DOT, or JSON node-edge pairs). This description is then rendered on the frontend into an interactive diagram.
Caching and Storage
Caching is central to minimizing LLM calls. Once generated, the structured description is stored in the Visual Snippets Database with fields:
•	topic_id
•	explanation_id
•	diagram_spec (Mermaid/DOT/JSON)
•	rendered_snapshot (optional cached image)
•	Metadata: timestamp, learner_id
On future access, the stored diagram specification is rendered directly. If learners provide negative feedback (e.g., unclear layout), the snippet can be regenerated with modified instructions.
Technical Approach
•	Best fit for MVP: LLM-generated diagram specifications rendered via frontend libraries such as Mermaid.js or D3.js.
•	Alternatives:
o	HuggingFace experimental text-to-diagram models.
o	Symbolic parsing of textbook equations into visual plots using matplotlib or networkx.
•	Database: Stores structured diagram descriptions for retrieval and reuse.
Interface
On the web platform, diagrams are shown as optional collapsible elements aligned with the explanation. On mobile devices, they are displayed as tappable thumbnails that expand into full-screen modals for clarity.
By embedding visual snippets, VidyaaSarthi introduces dual coding — reinforcing textual explanations with visual representations — while maintaining efficiency through structured caching and minimal reliance on LLMs.

Flashcards
o strengthen active recall, VidyaaSarthi generates flashcards from each personalized explanation. A flashcard presents a short question or prompt on one side and the corresponding answer on the reverse, enabling repeated self-testing and reinforcement of key concepts.
Generation Process
Candidate Q/A pairs are derived from the personalized explanation text using two complementary methods:
•	Keyword-driven extraction: nltk or spaCy identify definitional statements, processes, and causal relationships. Sentences with connectors such as “is,” “refers to,” or “leads to” often yield effective flashcard prompts.
•	LLM-based generation: the explanation is passed to the LLM with structured instructions, for example:
Generate 3–5 concise flashcards from the following explanation.  
Constraints:  
- Each flashcard should have a short question on one side and a one-sentence answer on the other.  
- Retain all key scientific terms.  
Explanation: <personalized_explanation>
Storage and Retrieval
Generated flashcards are cached in the Flashcard Database, with schema fields:
•	topic_id and explanation_id (linkage to source).
•	question_text and answer_text.
•	difficulty_level (optional metadata).
•	learner_profile_snapshot and timestamp.
On revisiting the passage, learners are served cached flashcards directly, minimizing LLM calls. If a learner’s profile is significantly updated after re-profiling, flashcards may be regenerated to align phrasing with the new profile.
Technical Approach
•	Best fit for MVP: LLM-prompted flashcard generation with caching.
•	Alternatives: HuggingFace QG (question generation) models such as t5-small or bart-large for extractive flashcard creation.
•	Storage: JSON-based Q/A pairs served directly to the frontend.
Interface
On the web platform, flashcards are presented as clickable interactive cards that flip to reveal the answer. On mobile devices, they appear as swipeable cards, with tap-to-reveal interactions optimized for touch screens. Learners may bookmark or mark cards for revision. Usage patterns such as time spent, number of attempts, and self-marked correctness are logged for analytics.
By embedding flashcards into the learning flow, VidyaaSarthi extends the value of personalized text beyond comprehension into long-term retention, combining syllabus fidelity with retrieval-based learning practices.
Practice Quizzes (Topic/Subtopic Level)
At the topic or subtopic level, VidyaaSarthi generates short practice quizzes directly derived from the personalized explanation. These quizzes provide immediate opportunities for learners to test comprehension of a concept and supply diagnostic signals for weak-area detection.
Generation Process
The quiz generation pipeline begins with the personalized explanation text, which is processed as follows:
•	Content parsing → nltk or spaCy extract key entities, definitions, and causal statements as candidate assessment points.
•	LLM-assisted generation → the parsed explanation is passed to the LLM with a structured prompt, e.g.:
Generate 3 multiple-choice questions, 2 true/false statements, and 3 short-answer questions (20–30 words expected) 
from the following explanation. Ensure syllabus fidelity, retain key terms, and match the learner’s grade level.
Explanation: <personalized_explanation>
The resulting quiz includes three formats:
•	MCQs → factual and conceptual recognition.
•	True/False → binary comprehension validation.
•	SAQs → short descriptive answers (20–30 words), requiring recall and articulation.
Storage and Retrieval
All generated items are cached in the Assessment Database with schema fields:
•	question_text
•	question_type (MCQ / True/False / SAQ)
•	options (if applicable)
•	expected_answer
•	topic_id, explanation_id, concept_tags
•	difficulty_level, timestamp
On revisiting the same topic, cached questions are retrieved directly. If the learner’s cognitive profile changes significantly after re-profiling, questions may be regenerated to align phrasing with the updated profile.
Evaluation and Feedback
•	MCQs and True/False → auto-graded against stored answers, with immediate visual feedback.
•	SAQs → evaluated using fuzzy matching (rapidfuzz, difflib) combined with synonym expansion (nltk.corpus.wordnet). For MVP, SAQs are matched against a set of acceptable reference answers; future versions may use semantic similarity scoring (sentence-transformers).
Learners receive outcome labels (correct, partially correct, incorrect) along with the ideal answer for reinforcement.
Weak-Area Tagging
Each question is tagged with one or more concept_tags (e.g., “Inputs of Photosynthesis,” “Newton’s Third Law”). Incorrect or partially correct responses flag these tags as potential weak areas.
•	A single error is logged but not over-weighted.
•	Repeated errors across questions with the same tag confirm a weak area.
Weak areas are surfaced in the learner’s dashboard with suggested remediation: links to personalized explanations, study notes, or flashcards.
Technical Approach
•	Best fit for MVP: LLM-prompted question generation with caching.
•	Alternatives: HuggingFace QG models (t5-small, bart-large) for extractive MCQs; rule-based extraction for definitional content.
•	Database: Assessment DB stores Q/A sets with concept tagging; learner attempts are logged for analytics.
Interface
On the web platform, quizzes appear inline beneath the personalized explanation with instant grading. On mobile devices, they are displayed as collapsible “Check Your Understanding” cards, with large tappable buttons for MCQs/True-False and compact text fields for SAQs. Learner responses and performance logs feed into both personal dashboards and teacher/parent reporting.
By combining structured item generation, immediate feedback, and concept-level tagging, VidyaaSarthi ensures that topic-level quizzes act as diagnostic learning units, enabling comprehension checks in real time.
Practice Quizzes (Chapter Level)
At the chapter scale, VidyaaSarthi delivers comprehensive quizzes that evaluate understanding across all subtopics. These assessments function both as revision tools and as diagnostic tests, consolidating knowledge while identifying weak concept clusters.
Generation Process
Chapter quizzes are assembled from the topic-level question pool. All cached MCQs, True/False items, and SAQs generated for individual topics are aggregated into a unified repository. The system applies selection and balancing rules:
•	Coverage → at least one question per subtopic.
•	Diversity → a mix of MCQs, True/False, and SAQs.
•	Difficulty balance → based on readability diagnostics and learner performance history, ensuring easy, moderate, and challenging items.
Where gaps exist (e.g., underrepresented concepts), VidyaaSarthi may trigger a supplementary LLM generation pass, instructed to create additional questions specifically for those concepts. These are tagged, cached, and added to the pool for future reuse.
Storage and Retrieval
Each chapter quiz is stored in the Assessment Database under a chapter_id. All questions retain their original concept_tags and metadata, enabling traceability. Quizzes can be served repeatedly without re-invoking the LLM, unless significant curriculum or learner-profile updates necessitate regeneration.
Evaluation and Feedback
•	MCQs and True/False → auto-graded with immediate marking.
•	SAQs → matched against a reference bank using fuzzy string comparison (rapidfuzz, difflib) and synonym expansion (nltk.corpus.wordnet).
•	Learners receive annotated responses: correct, partially correct, incorrect, plus the reference answer.
Diagnostic Layer
At the chapter level, diagnostics become more powerful:
•	Aggregated analysis → incorrect or partially correct answers are grouped by concept_tag.
•	Weak concept clusters → repeated errors across multiple subtopics confirm weak areas.
•	Chapter Diagnostic Report highlights:
o	Strong areas (concepts consistently correct).
o	Weak areas (concepts with repeated errors).
o	Remediation (links back to personalized explanations, study notes, flashcards, or topic-level quizzes).
Technical Approach
•	Best fit for MVP: LLM-prompted topic-level question generation + aggregation logic for chapter tests.
•	Alternatives: prebuilt item banks or HuggingFace QG models for enrichment.
•	Database: Assessment DB stores aggregated chapter quizzes with learner response logs for progress tracking.
Interface
On the web platform, chapter quizzes are presented in a dedicated “Chapter Test” module with progress indicators and a post-test summary dashboard. On mobile devices, quizzes are broken into swipeable sections for easy navigation, with diagnostic results displayed immediately after completion.
By aggregating topic-level assessments into structured chapter-level evaluations and analyzing responses through concept tagging, VidyaaSarthi ensures that learners receive both comprehensive revision and granular diagnostic insights, helping them focus effort where it is most needed.
System Flow: From Personalized Text to Revision Loop
The VidyaaSarthi learning pipeline integrates personalization, clarifications, derivative resources, quizzes, and diagnostics into a continuous cycle. Each component builds on the previous one, ensuring that learners not only understand new material but also reinforce it and address weak areas through targeted revision.
Step 1. Personalized Text
The process begins with text personalization. A textbook passage is analyzed for readability (using textstat) and rewritten by the LLM under explicit system-generated cues. The Final Cue combines readability-based simplification, cognitive-trait adjustments, and optional interest hooks. The personalized explanation is stored in the Personalized Content Database as the primary learning artifact.
Step 2. Clarification Questions & Responses
Immediately after engaging with the explanation, learners may ask clarification questions. VidyaaSarthi handles these through the Clarification Engine, which operates on a two-tier workflow:
•	Database-first retrieval → queries the Clarification Log using fuzzy string matching (rapidfuzz, difflib) and semantic similarity (sentence-transformers, text-embedding-ada-002). If a match is found, the stored answer is returned instantly.
•	LLM fallback → if no close match exists, the learner’s question, the original passage, the personalized explanation, and their profile are passed to the LLM. The model is instructed to provide a concise, syllabus-faithful response. The new Q/A pair is then cached in the Clarification Log Database for future reuse.
On the learner interface, clarifications are shown as expandable threads. On web, they appear in a side panel with search/scroll; on mobile, as a vertically expandable list under the explanation. This ensures learners can see not only their own clarifications but also previously logged Q&As relevant to the same topic.
Step 3. Derivative Learning Resources
From the personalized explanation, VidyaaSarthi generates supporting resources:
•	Study Notes → concise bullet points.
•	Visual Snippets → structured diagrams rendered with Mermaid.js or D3.js.
•	Flashcards → short Q/A pairs for active recall.
All resources are generated once, cached, and stored in structured databases for reuse.
Step 4. Topic-Level Quizzes
At the end of a topic, VidyaaSarthi generates a short quiz (MCQs, True/False, SAQs) from the personalized explanation. These are cached in the Assessment Database, tagged with concept_tags, and auto-graded or fuzzily evaluated. Incorrect or partial answers flag potential weak areas.
Step 5. Chapter-Level Quizzes
Topic-level quizzes are aggregated into chapter tests. The system ensures coverage across subtopics, diversity of formats, and difficulty balancing. Where gaps exist, the LLM supplements new items. A Chapter Diagnostic Report highlights strong areas, weak areas, and targeted remediation links.
Step 6. Weak-Area Diagnostics
Responses across topics and chapters are aggregated by concept_tag. Repeated errors confirm weak concepts, which are surfaced in learner dashboards and teacher/parent reports.
Step 7. Revision Loop
The system closes the loop by directing learners back to personalized explanations, study notes, flashcards, or clarifications for flagged weak concepts. The Clarification Engine remains accessible at all points in the loop, ensuring that recurring or new doubts are logged, reused, and resolved efficiently.
________________________________________
Through this continuous flow — Personalized Text → Clarifications → Derivative Resources → Topic Quizzes → Chapter Quizzes → Diagnostics → Revision Loop — VidyaaSarthi transforms static textbooks into an adaptive, syllabus-faithful learning system that improves comprehension, retention, and self-correction while minimizing unnecessary LLM usage.

 
Figure 1End-to-End VidyaaSarthi Learning Flow
A textbook passage is first transformed into a personalized explanation (Step 1). Learners may immediately ask clarification questions (Step 2), resolved first from the Clarification Log Database and only routed to the LLM when no match exists. From the explanation, the system generates derivative resources (notes, visuals, flashcards) and topic-level quizzes (Step 3–4). These build into chapter-level assessments and weak-area diagnostics (Step 5–6). Finally, the revision loop (Step 7) directs learners back to explanations, resources, or clarifications, creating a closed, adaptive cycle of comprehension, assessment, and remediation.
